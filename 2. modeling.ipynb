{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FosbOfwyU5BA"
   },
   "source": [
    "# 1. 디렉토리 및 라이브러리, 데이터 불러오기\n",
    "# 1. Set directory, get related data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1612676632026,
     "user": {
      "displayName": "성균관대심은주",
      "photoUrl": "",
      "userId": "14051116185193771540"
     },
     "user_tz": -540
    },
    "id": "70EJHV4cUpRn",
    "outputId": "5ef68ee0-a5f5-4c9d-b409-7ddbbe73b49e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y8cmSm9-VA6t"
   },
   "outputs": [],
   "source": [
    "# 전처리한 데이터 불러오기\n",
    "# Get preprocessed data (csv file)\n",
    "data = pd.read_csv('train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJDu0AH2V7-B"
   },
   "source": [
    "# 2. 모델링\n",
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbORq6l0V91w"
   },
   "source": [
    "#### 사용 모델 : LGBM \n",
    "#### 파라미터 튜닝 방법 : 너무 오래걸려서 손튜닝..^^\n",
    "\n",
    "#### Model: LGBM (Light GBM)\n",
    "#### Parameter tuning: Due to lack of time given + computationally taxing circumstances (severely), had attempted to use GridSearchCV only to resort to manual tuning at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYS5hinlW-kb"
   },
   "source": [
    "### a. 모델 훈련 및 성능 평가\n",
    "### a. Train model and check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j1Fsvp_lWdxo"
   },
   "outputs": [],
   "source": [
    "# 모델링을 위해 X변수와 target 변수로 나누어줌.\n",
    "# Split x,y data for modeling\n",
    "y = data['class']\n",
    "x = data.drop('class', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3jpcYbGXx3E"
   },
   "source": [
    "5-fold CV를 통해 모델의 모델의 성능 파악 및 반복 훈련\n",
    "Check overall performance of the model using 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63727,
     "status": "ok",
     "timestamp": 1612677384202,
     "user": {
      "displayName": "성균관대심은주",
      "photoUrl": "",
      "userId": "14051116185193771540"
     },
     "user_tz": -540
    },
    "id": "wZjSD1IOXvab",
    "outputId": "e0d88fd1-77d7-4726-c6ff-403313710bae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] seed is set=26, random_state=26 will be ignored. Current value: seed=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[351]\tvalid_0's auc: 0.999912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.99      1.00      7932\n",
      "     class 1       0.99      1.00      1.00      7802\n",
      "\n",
      "    accuracy                           1.00     15734\n",
      "   macro avg       1.00      1.00      1.00     15734\n",
      "weighted avg       1.00      1.00      1.00     15734\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] seed is set=26, random_state=26 will be ignored. Current value: seed=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[307]\tvalid_0's auc: 0.999478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.99      1.00      7898\n",
      "     class 1       0.99      1.00      1.00      7835\n",
      "\n",
      "    accuracy                           1.00     15733\n",
      "   macro avg       1.00      1.00      1.00     15733\n",
      "weighted avg       1.00      1.00      1.00     15733\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] seed is set=26, random_state=26 will be ignored. Current value: seed=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[763]\tvalid_0's auc: 0.999827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.99      1.00      7840\n",
      "     class 1       0.99      1.00      1.00      7893\n",
      "\n",
      "    accuracy                           1.00     15733\n",
      "   macro avg       1.00      1.00      1.00     15733\n",
      "weighted avg       1.00      1.00      1.00     15733\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] seed is set=26, random_state=26 will be ignored. Current value: seed=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[311]\tvalid_0's auc: 0.999809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.99      0.99      7849\n",
      "     class 1       0.99      1.00      0.99      7884\n",
      "\n",
      "    accuracy                           0.99     15733\n",
      "   macro avg       0.99      0.99      0.99     15733\n",
      "weighted avg       0.99      0.99      0.99     15733\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] seed is set=26, random_state=26 will be ignored. Current value: seed=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[302]\tvalid_0's auc: 0.999876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.99      1.00      7814\n",
      "     class 1       0.99      1.00      1.00      7919\n",
      "\n",
      "    accuracy                           1.00     15733\n",
      "   macro avg       1.00      1.00      1.00     15733\n",
      "weighted avg       1.00      1.00      1.00     15733\n",
      "\n",
      "F1 score : 0.995203\n"
     ]
    }
   ],
   "source": [
    "CM = []\n",
    "f1_scorea = []\n",
    "\n",
    "# 5개의 fold로 나누어줌\n",
    "# with n_splits=5, split data with 5 folds\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 26)\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(x)) :\n",
    "    #train data 만들기\n",
    "    #set train data\n",
    "    train_X, train_y = x.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    \n",
    "    #validation data 만들기\n",
    "    #set validation data\n",
    "    valid_X, valid_y = x.iloc[val_idx], y.iloc[val_idx]  \n",
    "    \n",
    "    # 손튜닝(..^^)을 통해 만든 파라미터 조합\n",
    "    # parameter values manually set\n",
    "    params = {'learning_rate': 0.3,\n",
    "              'num_iterations': 1000, \n",
    "              'max_depth': -1, \n",
    "              'boosting': 'gbdt', \n",
    "              'objective': 'binary', \n",
    "              'metric': 'auc', \n",
    "              'is_training_metric': True, \n",
    "              'num_leaves': 31, \n",
    "              'feature_fraction': 0.9, \n",
    "              'bagging_fraction': 1.0, \n",
    "              'bagging_freq': 5, \n",
    "              'seed':26}\n",
    "\n",
    "    # LGBM에서 사용한 데이터의 형식으로 바꾸어줌\n",
    "    # change format of datasets in order to apply LGBM modeling\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y) \n",
    "    valid_ds = lgb.Dataset(valid_X, label=valid_y) \n",
    "    \n",
    "    # 모델 생성 후 fitting\n",
    "    # fit after instantiating the model\n",
    "    model = lgb.LGBMClassifier(**params, random_state=26)\n",
    "    model.fit(train_X, train_y, eval_set=[(valid_X, valid_y)], early_stopping_rounds= 100, verbose=1000)\n",
    "\n",
    "    # validation set에 대해 y 예측\n",
    "    # predict y values regarding validation set defined\n",
    "    y_pred = model.predict(valid_X)\n",
    "\n",
    "    # confusion matrix\n",
    "    CM.append(confusion_matrix(valid_y, y_pred))\n",
    "    # F1 score\n",
    "    f1_scorea.append(f1_score(valid_y, y_pred))\n",
    "    #classification report: class 1에 대한 f1-score 확인하기 위해 report 출력\n",
    "    #check f1-score for class 1\n",
    "    print(classification_report(valid_y, y_pred, target_names=['class 0', 'class 1']))\n",
    "\n",
    "# 5개의 validation set에 대해 나온 confusion matrix 합쳐줌\n",
    "# combine 5 confusion matrices each from 5 different validation sets\n",
    "CM = sum(CM)\n",
    "\n",
    "# 5개의 validation set에 대한 f1-score 평균\n",
    "# average out f1-score for prediction of labels from 5 different validation sets\n",
    "f1_scorea = np.mean(f1_scorea)\n",
    "print(\"F1 score : %f\" % f1_scorea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1612677391467,
     "user": {
      "displayName": "성균관대심은주",
      "photoUrl": "",
      "userId": "14051116185193771540"
     },
     "user_tz": -540
    },
    "id": "wdd8X8ZYYpeJ",
    "outputId": "5693719b-6968-4a0e-ee4f-9e9b3fd2f95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38966 367 12 39321\n"
     ]
    }
   ],
   "source": [
    "# 5개 validation set에 대한 결과\n",
    "# results, 5 validation sets combined\n",
    "tn, fp, fn, tp = CM.ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1612677391871,
     "user": {
      "displayName": "성균관대심은주",
      "photoUrl": "",
      "userId": "14051116185193771540"
     },
     "user_tz": -540
    },
    "id": "zttAQfK5YxTR",
    "outputId": "2bbdfbb8-d3da-4cd0-f3e2-9ddfca8a05bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9670"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5개의 confusion matrix를 모두 합쳐 cost 계산\n",
    "# compute cost using results from 5 confusion matrices combined\n",
    "cost = fp*10 + fn*500\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kQVkFL0ZEmh"
   },
   "source": [
    "# 3. 최종 모델링 및 저장\n",
    "# 3. Final modeling and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1612680596267,
     "user": {
      "displayName": "성균관대심은주",
      "photoUrl": "",
      "userId": "14051116185193771540"
     },
     "user_tz": -540
    },
    "id": "bB9ww1tNZX59",
    "outputId": "689b9c90-7000-49df-8034-10a86a5e707b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LGBM.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 모델 저장\n",
    "# Save final model\n",
    "model = lgb.LGBMClassifier(learning_rate = 0.3,\n",
    "                        # 5-fold를 통해 600번의 iteration이면 충분하다고 생각해 변경\n",
    "                        # changed num_iterations\n",
    "                          num_iterations = 600, \n",
    "                          max_depth = -1, \n",
    "                          boosting = 'gbdt', \n",
    "                          objective = 'binary', \n",
    "                          metric = 'auc', \n",
    "                          is_training_metric = True, \n",
    "                          num_leaves = 31, \n",
    "                          feature_fraction = 0.9, \n",
    "                          bagging_fraction = 1.0, \n",
    "                          bagging_freq = 5, \n",
    "                          seed = 26)\n",
    " \n",
    "joblib.dump(model, 'LGBM.pkl') "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "모델링.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
